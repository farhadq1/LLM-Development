import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def combined_tfidf_match(data_row, rulebook_df, field_weight=0.5, business_name_weight=0.1, business_description_weight=0.1, high_level_category_weight=0.05, description_weight=0.05, example_weight=0.2, threshold=0.6):
    """
    Combines TF-IDF scores from field name, business name, business description (data_row) and
    high-level category, description, example (rulebook_df) to find the best match.

    Args:
        data_row: A row from the data_df DataFrame (containing 'field_cleaned', 'business_name_cleaned', 'business_description_cleaned').
        rulebook_df: The rulebook DataFrame (containing 'element_cleaned', 'high_level_category_cleaned', 'description_cleaned', 'example_cleaned').
        field_weight: Weight for the field name TF-IDF score.
        business_name_weight: Weight for the business name TF-IDF score.
        business_description_weight: Weight for the business description TF-IDF score.
        high_level_category_weight: Weight for the high-level category TF-IDF score.
        description_weight: Weight for the description TF-IDF score.
        example_weight: Weight for the example TF-IDF score.
        threshold: Minimum combined TF-IDF score to consider a match.

    Returns:
        The best matching rulebook element name, or None if no match exceeds the threshold.
    """

    vectorizer = TfidfVectorizer()

    # Combine text for each rulebook element
    rulebook_combined = rulebook_df['element_cleaned'].astype(str).tolist()

    # Combine text from the data row
    data_combined = [
        str(data_row['field_cleaned']),
        str(data_row['business_name_cleaned']),
        str(data_row['business_description_cleaned'])
    ]

    # Fit and transform the combined text
    tfidf_matrix = vectorizer.fit_transform(rulebook_combined + data_combined)

    # Calculate cosine similarities for each component
    field_similarities = cosine_similarity(tfidf_matrix[-3], tfidf_matrix[:-3]).flatten()
    business_name_similarities = cosine_similarity(tfidf_matrix[-2], tfidf_matrix[:-3]).flatten()
    business_description_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-3]).flatten()

    # Prepare rulebook features for weighted scoring
    def transform_and_weight(column, weight):
        return weight * column.astype(str).apply(lambda x: vectorizer.transform([x]).toarray().flatten())

    rulebook_features = (
        transform_and_weight(rulebook_df['high_level_category_cleaned'], high_level_category_weight) +
        transform_and_weight(rulebook_df['description_cleaned'], description_weight) +
        transform_and_weight(rulebook_df['example_cleaned'], example_weight)
    )

    # Calculate the weighted combined score
    combined_scores = (field_weight * field_similarities +
                       business_name_weight * business_name_similarities +
                       business_description_weight * business_description_similarities)

    # Add weighted rulebook features to the combined scores
    combined_scores += rulebook_features

    best_match_index = combined_scores.argmax()
    best_match_score = combined_scores[best_match_index]

    if best_match_score >= threshold:
        return rulebook_df.iloc[best_match_index]['element_name']
    else:
        return None

# Apply the combined matching function
data_df['best_match'] = data_df.apply(lambda row: combined_tfidf_match(row, rulebook_df), axis=1)

print(data_df[['field_name', 'business_name', 'business_description', 'best_match']])
