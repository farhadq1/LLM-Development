import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def combined_tfidf_match(data_row, rulebook_df, field_weight=0.5, business_name_weight=0.1, business_description_weight=0.1, high_level_category_weight=0.05, description_weight=0.05, example_weight=0.2, threshold=0.6):
    """
    Combines TF-IDF scores from field name, business name, business description (data_row) and
    high-level category, description, example (rulebook_df) to find the best match.

    Args:
        data_row: A row from the data_df DataFrame (containing 'field_cleaned', 'business_name_cleaned', 'business_description_cleaned').
        rulebook_df: The rulebook DataFrame (containing 'element_cleaned', 'high_level_category_cleaned', 'description_cleaned', 'example_cleaned').
        field_weight: Weight for the field name TF-IDF score.
        business_name_weight: Weight for the business name TF-IDF score.
        business_description_weight: Weight for the business description TF-IDF score.
        high_level_category_weight: Weight for the high-level category TF-IDF score.
        description_weight: Weight for the description TF-IDF score.
        example_weight: Weight for the example TF-IDF score.
        threshold: Minimum combined TF-IDF score to consider a match.

    Returns:
        The best matching rulebook element name, or None if no match exceeds the threshold.
    """

    vectorizer = TfidfVectorizer()

    # Combine text for each rulebook element
    rulebook_combined = rulebook_df['element_cleaned'].astype(str).tolist()

    # Combine text from the data row
    data_combined = [
        str(data_row['field_cleaned']),
        str(data_row['business_name_cleaned']),
        str(data_row['business_description_cleaned'])
    ]

    # Fit and transform the combined text
    tfidf_matrix = vectorizer.fit_transform(rulebook_combined + data_combined)

    # Calculate cosine similarities for each component
    field_similarities = cosine_similarity(tfidf_matrix[-3], tfidf_matrix[:-3]).flatten()
    business_name_similarities = cosine_similarity(tfidf_matrix[-2], tfidf_matrix[:-3]).flatten()
    business_description_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-3]).flatten()

    # Prepare rulebook features for weighted scoring
    def transform_and_weight(column, weight):
        transformed = column.astype(str).apply(lambda x: vectorizer.transform([x]).toarray().flatten())
        # Convert to a list of scalars
        return weight * np.array([np.mean(x) if len(x) > 0 else 0 for x in transformed])

    rulebook_features = (
        transform_and_weight(rulebook_df['high_level_category_cleaned'], high_level_category_weight) +
        transform_and_weight(rulebook_df['description_cleaned'], description_weight) +
        transform_and_weight(rulebook_df['example_cleaned'], example_weight)
    )

    # Calculate the weighted combined score
    combined_scores = (field_weight * field_similarities +
                       business_name_weight * business_name_similarities +
                       business_description_weight * business_description_similarities)

    # Add weighted rulebook features to the combined scores
    combined_scores += rulebook_features

    best_match_index = combined_scores.argmax()
    best_match_score = combined_scores[best_match_index]

    if best_match_score >= threshold:
        return rulebook_df.iloc[best_match_index]['element_name']
    else:
        return None

# Apply the combined matching function
data_df['best_match'] = data_df.apply(lambda row: combined_tfidf_match(row, rulebook_df), axis=1)


import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# --- 1. Preprocessing and TF-IDF Setup (Run once before applying to data_df) ---

def preprocess_data_for_tfidf(data_df, rulebook_df):
    """
    Preprocesses text data and initializes TF-IDF vectorizer and rulebook features.
    """
    # Combine all relevant text for fitting the vectorizer
    all_text = (
        rulebook_df['element_cleaned'].astype(str).tolist() +
        rulebook_df['high_level_category_cleaned'].astype(str).tolist() +
        rulebook_df['description_cleaned'].astype(str).tolist() +
        rulebook_df['example_cleaned'].astype(str).tolist() +
        data_df['field_cleaned'].astype(str).tolist() +
        data_df['business_name_cleaned'].astype(str).tolist() +
        data_df['business_description_cleaned'].astype(str).tolist()
    )

    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2)) # Example: add stop words, n-grams
    vectorizer.fit(all_text) # Fit the vectorizer on the entire corpus

    # Pre-compute TF-IDF for rulebook elements (element_cleaned)
    rulebook_elements_tfidf = vectorizer.transform(rulebook_df['element_cleaned'].astype(str).tolist())

    # Pre-compute weighted rulebook features
    # Helper for pre-computing rulebook feature TF-IDF and weighted sums
    def get_weighted_rulebook_feature_tfidf(column, weight, vectorizer):
        transformed_features = vectorizer.transform(column.astype(str).tolist())
        # Calculate mean TF-IDF score for each document (row)
        # Using .mean(axis=1) is more efficient for sparse matrices if applicable,
        # but for individual values, this approach is fine.
        # For sparse matrices, sum/count of non-zero elements is more robust than direct mean.
        # Here, we're assuming dense array for simplicity as per original.
        return weight * np.array([np.mean(vec.toarray()) if vec.nnz > 0 else 0 for vec in transformed_features])

    rulebook_features_precomputed = (
        get_weighted_rulebook_feature_tfidf(rulebook_df['high_level_category_cleaned'], 0.05, vectorizer) +
        get_weighted_rulebook_feature_tfidf(rulebook_df['description_cleaned'], 0.05, vectorizer) +
        get_weighted_rulebook_feature_tfidf(rulebook_df['example_cleaned'], 0.2, vectorizer)
    )

    return vectorizer, rulebook_elements_tfidf, rulebook_features_precomputed

# --- 2. Improved Matching Function ---

def combined_tfidf_match_optimized(
    data_row,
    vectorizer,
    rulebook_elements_tfidf,
    rulebook_features_precomputed,
    rulebook_element_names, # Pass element names for lookup
    field_weight=0.5,
    business_name_weight=0.1,
    business_description_weight=0.1,
    threshold=0.6
):
    """
    Combines TF-IDF scores using pre-computed rulebook features for efficiency.

    Args:
        data_row: A row from the data_df DataFrame.
        vectorizer: The pre-fitted TfidfVectorizer.
        rulebook_elements_tfidf: Pre-computed TF-IDF matrix for rulebook element names.
        rulebook_features_precomputed: Pre-computed and weighted rulebook feature scores.
        rulebook_element_names: List/Series of rulebook element names for return value.
        # ... (weights and threshold)
    """

    # Transform data row components using the pre-fitted vectorizer
    field_tfidf = vectorizer.transform([str(data_row['field_cleaned'])])
    business_name_tfidf = vectorizer.transform([str(data_row['business_name_cleaned'])])
    business_description_tfidf = vectorizer.transform([str(data_row['business_description_cleaned'])])

    # Calculate cosine similarities
    field_similarities = cosine_similarity(field_tfidf, rulebook_elements_tfidf).flatten()
    business_name_similarities = cosine_similarity(business_name_tfidf, rulebook_elements_tfidf).flatten()
    business_description_similarities = cosine_similarity(business_description_tfidf, rulebook_elements_tfidf).flatten()

    # Calculate the weighted combined score
    combined_scores = (
        field_weight * field_similarities +
        business_name_weight * business_name_similarities +
        business_description_weight * business_description_similarities
    )

    # Add the pre-computed weighted rulebook features
    combined_scores += rulebook_features_precomputed

    best_match_index = combined_scores.argmax()
    best_match_score = combined_scores[best_match_index]

    if best_match_score >= threshold:
        return rulebook_element_names[best_match_index]
    else:
        return None

# --- Example Usage ---

# Assuming data_df and rulebook_df are already loaded and cleaned
# Example dummy data for demonstration:
data = {'field_cleaned': ['customer name', 'product category', 'transaction date'],
        'business_name_cleaned': ['ABC Corp', 'XYZ Inc', 'PQR Ltd'],
        'business_description_cleaned': ['sells software', 'e-commerce platform', 'financial services']}
data_df = pd.DataFrame(data)

rule_data = {'element_name': ['Client_ID', 'Item_Category', 'Date_of_Sale'],
             'element_cleaned': ['client identifier', 'product group', 'sales date'],
             'high_level_category_cleaned': ['Customer Data', 'Product Data', 'Financial Data'],
             'description_cleaned': ['Unique ID for customer', 'Grouping of products', 'When sale occurred'],
             'example_cleaned': ['C001, John Doe', 'Electronics, Books', '2023-01-15, 01/01/2023']}
rulebook_df = pd.DataFrame(rule_data)

# Step 1: Preprocess and get pre-computed values
vectorizer, rulebook_elements_tfidf, rulebook_features_precomputed = preprocess_data_for_tfidf(data_df, rulebook_df)
rulebook_element_names = rulebook_df['element_name'].tolist() # Get this once

# Step 2: Apply the optimized matching function
data_df['best_match'] = data_df.apply(
    lambda row: combined_tfidf_match_optimized(
        row,
        vectorizer,
        rulebook_elements_tfidf,
        rulebook_features_precomputed,
        rulebook_element_names # Pass the element names here
    ),
    axis=1
)

print(data_df)
